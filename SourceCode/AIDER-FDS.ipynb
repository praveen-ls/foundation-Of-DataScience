{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1c429-7475-4afc-9a2a-a6c9684507f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "\n",
    "# Define paths\n",
    "original_data_dir = 'E:\\\\FDS\\\\Project\\\\AIDER'  \n",
    "train_path = \"E:\\\\FDS\\\\Project\\\\train\"\n",
    "test_path = \"E:\\\\FDS\\\\Project\\\\test\"\n",
    "TARGET_SIZE = (256, 256)  \n",
    "\n",
    "# Split into Train and Test (with resizing)\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "for class_name in os.listdir(original_data_dir):\n",
    "    class_path = os.path.join(original_data_dir, class_name)\n",
    "    images = glob.glob(os.path.join(class_path, \"*.jpg\"))  \n",
    "\n",
    "    train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "    os.makedirs(os.path.join(train_path, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_path, class_name), exist_ok=True)\n",
    "\n",
    "    for img_path in train_images:\n",
    "        img = load_img(img_path)\n",
    "        img = img.resize(TARGET_SIZE)  \n",
    "        img.save(os.path.join(train_path, class_name, os.path.basename(img_path)))\n",
    "\n",
    "    for img_path in test_images:\n",
    "        img = load_img(img_path)\n",
    "        img = img.resize(TARGET_SIZE)  \n",
    "        img.save(os.path.join(test_path, class_name, os.path.basename(img_path)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ddf11-398e-49b9-8882-b3b294b7e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting collapsed_building from 408 to 3200 images...\n",
      "Augmenting fire from 416 to 3200 images...\n",
      "Augmenting flooded_areas from 420 to 3200 images...\n",
      "Skipping normal, already has 3512 images.\n",
      "Augmenting traffic_incident from 388 to 3200 images...\n",
      "✅ Augmentation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "\n",
    "def augment_images(class_name, target_count):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, \n",
    "        shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    class_train_path = os.path.join(train_path, class_name)\n",
    "    images = glob.glob(os.path.join(class_train_path, \"*.jpg\"))\n",
    "\n",
    "    current_count = len(images)\n",
    "    if current_count >= target_count:\n",
    "        print(f\"Skipping {class_name}, already has {current_count} images.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Augmenting {class_name} from {current_count} to {target_count} images...\")\n",
    "\n",
    "    generated = 0  \n",
    "    images_per_original = max(1, (target_count - current_count) // current_count) \n",
    "\n",
    "    for img_path in images:\n",
    "        if current_count + generated >= target_count:\n",
    "            break \n",
    "\n",
    "        img = load_img(img_path)\n",
    "        img = img.resize(TARGET_SIZE)  \n",
    "        img = img_to_array(img)\n",
    "        img = img.reshape((1,) + img.shape)\n",
    "\n",
    "        save_prefix = os.path.basename(img_path).split(\".\")[0] + \"_aug\"\n",
    "        i = 0\n",
    "        for batch in datagen.flow(img, batch_size=1, save_to_dir=class_train_path, \n",
    "                                  save_prefix=save_prefix, save_format=\"jpg\"):\n",
    "            generated += 1\n",
    "            i += 1\n",
    "            if current_count + generated >= target_count or i >= images_per_original:\n",
    "                break  \n",
    "\n",
    "# Apply augmentation to all classes\n",
    "for class_name in os.listdir(train_path):\n",
    "    augment_images(class_name, 3200)\n",
    "\n",
    "print(\"✅ Augmentation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8cfb2d-98c6-4270-bf26-04151e0a38b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15323 images belonging to 5 classes.\n",
      "Found 1289 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prave\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\prave\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1187s\u001b[0m 2s/step - accuracy: 0.4458 - loss: 1.3530 - val_accuracy: 0.6144 - val_loss: 1.0330\n",
      "Epoch 2/10\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1139s\u001b[0m 2s/step - accuracy: 0.7457 - loss: 0.6941 - val_accuracy: 0.7215 - val_loss: 0.7807\n",
      "Epoch 4/10\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1113s\u001b[0m 2s/step - accuracy: 0.7952 - loss: 0.5705 - val_accuracy: 0.8045 - val_loss: 0.5888\n",
      "Epoch 5/10\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1205s\u001b[0m 3s/step - accuracy: 0.8498 - loss: 0.4192 - val_accuracy: 0.7898 - val_loss: 0.6794\n",
      "Epoch 6/10\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1149s\u001b[0m 2s/step - accuracy: 0.9083 - loss: 0.2572 - val_accuracy: 0.8239 - val_loss: 0.6074\n",
      "Epoch 8/10\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1137s\u001b[0m 2s/step - accuracy: 0.9250 - loss: 0.2040 - val_accuracy: 0.8324 - val_loss: 0.6726\n",
      "Epoch 9/10\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1301s\u001b[0m 3s/step - accuracy: 0.9314 - loss: 0.1886 - val_accuracy: 0.8278 - val_loss: 0.7094\n",
      "Epoch 10/10\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1380s\u001b[0m 3s/step - accuracy: 0.9485 - loss: 0.1540 - val_accuracy: 0.8154 - val_loss: 0.7794\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 542ms/step - accuracy: 0.7976 - loss: 0.8227\n",
      "Validation Accuracy: 81.54%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load Train & Test Data using ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)  \n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'  \n",
    ")\n",
    "\n",
    "val_gen = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Define CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(5, activation='softmax')  \n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    train_gen, \n",
    "    validation_data=val_gen,  \n",
    "    epochs=10,  \n",
    "    steps_per_epoch=len(train_gen),  \n",
    "    validation_steps=len(val_gen)  \n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, accuracy = model.evaluate(val_gen)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e8af0-0d16-4df1-a855-56dbc3dae82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
